## 量化有哪些方法？分别什么特点

模型量化分类：

- 按量化方式分，有后训练量化`PTQ`和量化感知训练`QAT`
- 按量化时是否需要校准数据，是否量化激活值分，有静态量化和动态量化

由于训练后量化，相对简单高效，只需要已训练好的模型 + 少量校准数据，无需重新训练模型，所以使用较为广泛。
根据是否量化激活又分为动态量化和静态量化，下面将详细介绍两种方法的区别。

模型的量化对象主要分权重和激活两类：<br>
权重：训练完后固定，数值范围(range)与输入无关，可离线完成量化，通常相对容易量化<br>
激活：激活输出随输入变化而变化，需要统计数据动态范围，通常更难量化。

**动态量化(PTQ dynamic)**

动态量化由于仅量化权重，也称为权重量化(`weight-only quantization`)，其激活在推理时进行量化，适用于`LSTM、MLP、Transformer`等模型，而静态量化则适用于`CNN`。

![image](https://github.com/googlehjx/LLM_Knowledge_QA/blob/main/img/%E5%8A%A8%E6%80%81%E9%87%8F%E5%8C%96.jpeg)

上图是`PTQ`动态模型量化前后的结构对比，只有`weight`被提前量化成`INT8`。在推理阶段首先将量化的权重反量化为浮点形式，推理过程仍然为浮点计算，无法加速推理过程。

**静态量化(PTQ static)**

静态量化则是权重和激活都提前量化，也称为**全量化**，不仅可以压缩模型大小，减少推理过程的内存占用，而且因为激活值和权重都为整型数据，
因此可以使用高效的整型运算单元加速推理过程。为了量化激活，需要使用具有代表性的数据进行推理，然后统计各个激活的数据范围，这个步骤也称为**校准**(`Calibration`)

![image](https://github.com/googlehjx/LLM_Knowledge_QA/blob/main/img/%E9%9D%99%E6%80%81%E9%87%8F%E5%8C%96.jpeg)

### AWQ量化方法
AWQ是一种对模型权重进行低比特量化的方法，使用该方法可以将模型权重(Weight)量化为4bit，并在计算激活值(Activation)时反量化为FP16，即W4A16。也可以基于AWQ方法将权重量化为3bit/8bit，并在计算时是使用4bit/8bit/16bit，由此衍生出W4A4、W4A8等一系列方法。它是PTQ量化方式的一种。

**核心观点1：权重并不同等重要，仅有小部分显著权重对推理结果影响较大** 

AWQ论文作者指出，模型的权重并不同等重要，仅有0.1%~1%的权重参数对模型输出精度影响较大，称为显著权重`salient weights`。因此保持这部分权重为原来的精度(FP16)，对其他权重进行低比特量化，就可以在保持精度几乎不变的情况下，大幅降低模型内存占用，并提升推理速度。

AWQ提出的方法是通过激活值作为参考，识别出哪些参数属于显著权重，量化后模型效果最佳。而**激活**就是一个处理模块的输入，如计算注意力时，输入X就是激活值。

**核心观点2：量化时对显著权重进行放大可以降低量化误差**

考虑一个权重矩阵`W`，线性运算可以写作`y = Wx`。对权重矩阵进行量化后，可以写作`y = Q(W)x`，量化函数Q(·)定义为公式1：

$$
\begin{aligned}
&Q(W) = Δ·Round(\frac w Δ)\\
&Δ = \frac{Max(\vert{W}\vert)}{2^{N-1}}
\end{aligned}
$$

其中`N`是量化后的比特数，Δ是量化因子(scaler)。$`w' = Round(\frac w Δ)`$是量化过程，$`Δ·w'`$是反量化过程。
